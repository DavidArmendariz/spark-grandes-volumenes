{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# ST1803 Usando PySpark\n",
        "\n",
        "Vamos a configurar el uso de PySpark en el cuaderno (dirigido principalmente para el uso en Google Colab) y haremos algunos ejemplos de calentamiento.\n",
        "\n",
        "El código de configuración tomado del repositorio https://github.com/groda/big_data/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hknPATlqgiXd"
      },
      "source": [
        "Revisar si se tiene Java 8 o posterior. En Colab tenemos el último Java (11) pero en otros ambientes deberá ser instalado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCBODIski2x"
      },
      "source": [
        "## Ejemplo1: Hello World\n",
        "\n",
        "Vamos a empezar con una aplicación que:\n",
        "\n",
        "- Comience una sesión de Spark llamada `spark`\n",
        "- Imprima \"Hello, World!\"\n",
        "- Cierre la sesión de Spark.\n",
        "\n",
        "Esta sería una aplicación auto-contenida (ver https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eevSjgIEoaNh",
        "outputId": "c8e92d4b-ea8a-4f4b-bf6d-c19415d7c7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting HelloWorld.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile HelloWorld.py\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Hello World\").getOrCreate()\n",
        "\n",
        "print(\"Hello, World!\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzN0YKnFrPia"
      },
      "source": [
        "Para ejecutar una aplicación en Spark se puede utilizar directamente Python, pero si se quiere utilizar toda la configuración de Spark debe usar el script `spark-submit`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKYobzFolpR",
        "outputId": "b1fa8d1a-ce20-4f64-e6eb-7ce6f3be4e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/02/21 18:48:44 WARN Utils: Your hostname, Davids-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.161.51.44 instead (on interface en0)\n",
            "24/02/21 18:48:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/02/21 18:48:44 INFO SparkContext: Running Spark version 3.5.0\n",
            "24/02/21 18:48:44 INFO SparkContext: OS info Mac OS X, 14.3.1, aarch64\n",
            "24/02/21 18:48:44 INFO SparkContext: Java version 20.0.2\n",
            "24/02/21 18:48:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/21 18:48:45 INFO ResourceUtils: ==============================================================\n",
            "24/02/21 18:48:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/21 18:48:45 INFO ResourceUtils: ==============================================================\n",
            "24/02/21 18:48:45 INFO SparkContext: Submitted application: Hello World\n",
            "24/02/21 18:48:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/21 18:48:45 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/21 18:48:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/21 18:48:45 INFO SecurityManager: Changing view acls to: david\n",
            "24/02/21 18:48:45 INFO SecurityManager: Changing modify acls to: david\n",
            "24/02/21 18:48:45 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/21 18:48:45 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/21 18:48:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: david; groups with view permissions: EMPTY; users with modify permissions: david; groups with modify permissions: EMPTY\n",
            "24/02/21 18:48:45 INFO Utils: Successfully started service 'sparkDriver' on port 51827.\n",
            "24/02/21 18:48:45 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/21 18:48:45 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/21 18:48:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/21 18:48:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/21 18:48:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/21 18:48:45 INFO DiskBlockManager: Created local directory at /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/blockmgr-63d918cc-269a-4182-b00d-3e4e8ff6c15f\n",
            "24/02/21 18:48:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/21 18:48:45 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/21 18:48:45 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/21 18:48:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/21 18:48:45 INFO Executor: Starting executor ID driver on host 10.161.51.44\n",
            "24/02/21 18:48:45 INFO Executor: OS info Mac OS X, 14.3.1, aarch64\n",
            "24/02/21 18:48:45 INFO Executor: Java version 20.0.2\n",
            "24/02/21 18:48:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/21 18:48:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2c00c601 for default.\n",
            "24/02/21 18:48:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51828.\n",
            "24/02/21 18:48:45 INFO NettyBlockTransferService: Server created on 10.161.51.44:51828\n",
            "24/02/21 18:48:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/21 18:48:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.161.51.44, 51828, None)\n",
            "24/02/21 18:48:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.161.51.44:51828 with 434.4 MiB RAM, BlockManagerId(driver, 10.161.51.44, 51828, None)\n",
            "24/02/21 18:48:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.161.51.44, 51828, None)\n",
            "24/02/21 18:48:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.161.51.44, 51828, None)\n",
            "Hello, World!\n",
            "24/02/21 18:48:45 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/02/21 18:48:45 INFO SparkUI: Stopped Spark web UI at http://10.161.51.44:4040\n",
            "24/02/21 18:48:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/02/21 18:48:45 INFO MemoryStore: MemoryStore cleared\n",
            "24/02/21 18:48:45 INFO BlockManager: BlockManager stopped\n",
            "24/02/21 18:48:45 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/02/21 18:48:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/02/21 18:48:45 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/02/21 18:48:46 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/21 18:48:46 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-f766a2ab-87cf-455c-a1d5-49a9b00e9efb/pyspark-384bee07-62bf-4789-92f6-96f56246a9d0\n",
            "24/02/21 18:48:46 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-f766a2ab-87cf-455c-a1d5-49a9b00e9efb\n",
            "24/02/21 18:48:46 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-8c80afb6-0443-4917-bd98-efd347d5f436\n"
          ]
        }
      ],
      "source": [
        "!spark-submit HelloWorld.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brBqIRlJrd0A"
      },
      "source": [
        "Todo el texto adicional de \"Hello, World!\" es debido a mensajes logs de la plataforma. Si se quiere tener estos logs aparte los puedo llegar a un archivo (por defecto van al stream estándar de errores).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQP0a0CXoyQY",
        "outputId": "a7299d3e-d4c4-478f-f0aa-3d8d1331f304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "!spark-submit HelloWorld.py 2>log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm4ijR2hrwHt"
      },
      "source": [
        "Ahora tengo los logs separados, que puedo revisar en el archivo `log.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbg5z3eOr40g",
        "outputId": "1fe15727-010e-4e56-cd33-8921dabcf8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/02/21 18:48:46 WARN Utils: Your hostname, Davids-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.161.51.44 instead (on interface en0)\n",
            "24/02/21 18:48:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/02/21 18:48:47 INFO SparkContext: Running Spark version 3.5.0\n",
            "24/02/21 18:48:47 INFO SparkContext: OS info Mac OS X, 14.3.1, aarch64\n",
            "24/02/21 18:48:47 INFO SparkContext: Java version 20.0.2\n",
            "24/02/21 18:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/21 18:48:47 INFO ResourceUtils: ==============================================================\n",
            "24/02/21 18:48:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/21 18:48:47 INFO ResourceUtils: ==============================================================\n",
            "24/02/21 18:48:47 INFO SparkContext: Submitted application: Hello World\n",
            "24/02/21 18:48:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/21 18:48:47 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/21 18:48:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/21 18:48:47 INFO SecurityManager: Changing view acls to: david\n",
            "24/02/21 18:48:47 INFO SecurityManager: Changing modify acls to: david\n",
            "24/02/21 18:48:47 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/21 18:48:47 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/21 18:48:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: david; groups with view permissions: EMPTY; users with modify permissions: david; groups with modify permissions: EMPTY\n",
            "24/02/21 18:48:47 INFO Utils: Successfully started service 'sparkDriver' on port 51833.\n",
            "24/02/21 18:48:47 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/21 18:48:47 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/21 18:48:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/21 18:48:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/21 18:48:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/21 18:48:47 INFO DiskBlockManager: Created local directory at /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/blockmgr-16d633d3-776f-4193-97bf-afdec28a75c6\n",
            "24/02/21 18:48:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/21 18:48:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/21 18:48:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/21 18:48:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/21 18:48:47 INFO Executor: Starting executor ID driver on host 10.161.51.44\n",
            "24/02/21 18:48:47 INFO Executor: OS info Mac OS X, 14.3.1, aarch64\n",
            "24/02/21 18:48:47 INFO Executor: Java version 20.0.2\n",
            "24/02/21 18:48:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/21 18:48:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4995aab3 for default.\n",
            "24/02/21 18:48:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51834.\n",
            "24/02/21 18:48:47 INFO NettyBlockTransferService: Server created on 10.161.51.44:51834\n",
            "24/02/21 18:48:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/21 18:48:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.161.51.44, 51834, None)\n",
            "24/02/21 18:48:47 INFO BlockManagerMasterEndpoint: Registering block manager 10.161.51.44:51834 with 434.4 MiB RAM, BlockManagerId(driver, 10.161.51.44, 51834, None)\n",
            "24/02/21 18:48:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.161.51.44, 51834, None)\n",
            "24/02/21 18:48:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.161.51.44, 51834, None)\n",
            "24/02/21 18:48:48 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/02/21 18:48:48 INFO SparkUI: Stopped Spark web UI at http://10.161.51.44:4040\n",
            "24/02/21 18:48:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/02/21 18:48:48 INFO MemoryStore: MemoryStore cleared\n",
            "24/02/21 18:48:48 INFO BlockManager: BlockManager stopped\n",
            "24/02/21 18:48:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/02/21 18:48:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/02/21 18:48:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/02/21 18:48:48 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/21 18:48:48 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-9a663a89-ae12-4b60-9c89-a866e5d4873a\n",
            "24/02/21 18:48:48 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-7b2651bd-fb51-4c59-9a03-489c83d71e8a/pyspark-c8a3ea36-b294-4570-97b8-56aea8e47f8b\n",
            "24/02/21 18:48:48 INFO ShutdownHookManager: Deleting directory /private/var/folders/nz/j6px70h134sbbj9vwy0gpzm40000gn/T/spark-7b2651bd-fb51-4c59-9a03-489c83d71e8a\n"
          ]
        }
      ],
      "source": [
        "!cat log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjJFIEHossMN"
      },
      "source": [
        "Pero ejecutar la aplicación se sintió muy lenta, la razón de la mayoría de la lentitud tiene que ver con la máquina virtual de Java (JVM en inglés), la cual debe ejecutarse y luego el motor de Spark se ejecuta sobre esta. Veamos cuánto toma sólo la aplicación en ejecutarse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siRB48IWtLhH",
        "outputId": "9b28c54e-8fab-4c92-b8e0-db70a7e7edba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, World!\n",
            "CPU times: user 18.6 ms, sys: 8.65 ms, total: 27.2 ms\n",
            "Wall time: 2.38 s\n"
          ]
        }
      ],
      "source": [
        "%time !spark-submit HelloWorld.py 2>log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mrRsz8t9Jt"
      },
      "source": [
        "## Ejemplos en PySpark\n",
        "\n",
        "PySpark viene con muchos ejemplos en su instalación, para encontrarlos hay que saber dónde quedó instalado PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAEurL1DuGZS",
        "outputId": "c349d0e1-d6b6-44ab-ede3-67dd60826223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.0\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /Users/david/.local/share/virtualenvs/spark-grandes-volumenes-sdSE_HtW/lib/python3.11/site-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ilbckEuI0i"
      },
      "source": [
        "/usr/local/lib/python3.10/dist-packages es donde quedó instalado, debemos buscar la carpeta `examples`. Otra forma de saber donde está instalado es usar el archivo `find_spark_home.py` y usarlo para crear una variable de ambiente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9POsJU1auzgn",
        "outputId": "62a63846-81c1-4b41-9922-7441bdcce261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/david/.local/share/virtualenvs/spark-grandes-volumenes-sdSE_HtW/lib/python3.11/site-packages/pyspark\n"
          ]
        }
      ],
      "source": [
        "!find_spark_home.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvPktN_dvCZr",
        "outputId": "de3c4d92-1306-4751-9a25-09d5492e53d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carpeta de PySpark en: /Users/david/.local/share/virtualenvs/spark-grandes-volumenes-sdSE_HtW/lib/python3.11/site-packages/pyspark\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "pyspark_folder = subprocess.run([\"find_spark_home.py\"], capture_output=True, text=True)\n",
        "print(\"Carpeta de PySpark en:\", pyspark_folder.stdout)\n",
        "# Resultado en una variable de entorno\n",
        "os.environ[\"SPARK_HOME\"] = pyspark_folder.stdout.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMfjPMyfviJf",
        "outputId": "8cc7ed24-23c7-4ffd-e5ad-9f67d6cbb16a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py               install.py                \u001b[1m\u001b[36msbin\u001b[m\u001b[m/\n",
            "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m/              instrumentation_utils.py  serializers.py\n",
            "_globals.py               \u001b[1m\u001b[36mjars\u001b[m\u001b[m/                     shell.py\n",
            "_typing.pyi               java_gateway.py           shuffle.py\n",
            "accumulators.py           join.py                   \u001b[1m\u001b[36msql\u001b[m\u001b[m/\n",
            "\u001b[1m\u001b[36mbin\u001b[m\u001b[m/                      \u001b[1m\u001b[36mlicenses\u001b[m\u001b[m/                 statcounter.py\n",
            "broadcast.py              \u001b[1m\u001b[36mml\u001b[m\u001b[m/                       status.py\n",
            "\u001b[1m\u001b[36mcloudpickle\u001b[m\u001b[m/              \u001b[1m\u001b[36mmllib\u001b[m\u001b[m/                    storagelevel.py\n",
            "conf.py                   \u001b[1m\u001b[36mpandas\u001b[m\u001b[m/                   \u001b[1m\u001b[36mstreaming\u001b[m\u001b[m/\n",
            "context.py                profiler.py               taskcontext.py\n",
            "daemon.py                 py.typed                  \u001b[1m\u001b[36mtesting\u001b[m\u001b[m/\n",
            "\u001b[1m\u001b[36mdata\u001b[m\u001b[m/                     \u001b[1m\u001b[36mpython\u001b[m\u001b[m/                   traceback_utils.py\n",
            "\u001b[1m\u001b[36merrors\u001b[m\u001b[m/                   rdd.py                    util.py\n",
            "\u001b[1m\u001b[36mexamples\u001b[m\u001b[m/                 rddsampler.py             version.py\n",
            "files.py                  \u001b[1m\u001b[36mresource\u001b[m\u001b[m/                 worker.py\n",
            "find_spark_home.py        resultiterable.py         worker_util.py\n"
          ]
        }
      ],
      "source": [
        "!ls -p $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptkrt6EzwIrn",
        "outputId": "f150b119-a7f5-4ebe-f7cf-7fdc10bcda4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/Users/david/.local/share/virtualenvs/spark-grandes-volumenes-sdSE_HtW/lib/python3.11/site-packages/pyspark/examples\u001b[0m\n",
            "└── \u001b[01;34msrc\u001b[0m\n",
            "    └── \u001b[01;34mmain\u001b[0m\n",
            "        └── \u001b[01;34mpython\u001b[0m\n",
            "            ├── \u001b[00m__init__.py\u001b[0m\n",
            "            ├── \u001b[00mals.py\u001b[0m\n",
            "            ├── \u001b[00mavro_inputformat.py\u001b[0m\n",
            "            ├── \u001b[00mkmeans.py\u001b[0m\n",
            "            ├── \u001b[00mlogistic_regression.py\u001b[0m\n",
            "            ├── \u001b[01;34mml\u001b[0m\n",
            "            │   ├── \u001b[00maft_survival_regression.py\u001b[0m\n",
            "            │   ├── \u001b[00mals_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mbinarizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mbisecting_k_means_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mbucketed_random_projection_lsh_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mbucketizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mchi_square_test_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mchisq_selector_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mcorrelation_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mcount_vectorizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mcross_validator.py\u001b[0m\n",
            "            │   ├── \u001b[00mdataframe_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mdct_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mdecision_tree_classification_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mdecision_tree_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00melementwise_product_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mestimator_transformer_param_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mfeature_hasher_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mfm_classifier_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mfm_regressor_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mfpgrowth_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgaussian_mixture_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgeneralized_linear_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgradient_boosted_tree_classifier_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgradient_boosted_tree_regressor_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mimputer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mindex_to_string_example.py\u001b[0m\n",
            "            │   ├── \u001b[00minteraction_example.py\u001b[0m\n",
            "            │   ├── \u001b[00misotonic_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mkmeans_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mlda_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mlinear_regression_with_elastic_net.py\u001b[0m\n",
            "            │   ├── \u001b[00mlinearsvc.py\u001b[0m\n",
            "            │   ├── \u001b[00mlogistic_regression_summary_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mlogistic_regression_with_elastic_net.py\u001b[0m\n",
            "            │   ├── \u001b[00mmax_abs_scaler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mmin_hash_lsh_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mmin_max_scaler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mmulticlass_logistic_regression_with_elastic_net.py\u001b[0m\n",
            "            │   ├── \u001b[00mmultilayer_perceptron_classification.py\u001b[0m\n",
            "            │   ├── \u001b[00mn_gram_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mnaive_bayes_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mnormalizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mone_vs_rest_example.py\u001b[0m\n",
            "            │   ├── \u001b[00monehot_encoder_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpca_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpipeline_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpolynomial_expansion_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpower_iteration_clustering_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mprefixspan_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mquantile_discretizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrandom_forest_classifier_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrandom_forest_regressor_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrformula_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrobust_scaler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00msql_transformer.py\u001b[0m\n",
            "            │   ├── \u001b[00mstandard_scaler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mstopwords_remover_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mstring_indexer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00msummarizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mtf_idf_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mtokenizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mtrain_validation_split.py\u001b[0m\n",
            "            │   ├── \u001b[00munivariate_feature_selector_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mvariance_threshold_selector_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mvector_assembler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mvector_indexer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mvector_size_hint_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mvector_slicer_example.py\u001b[0m\n",
            "            │   └── \u001b[00mword2vec_example.py\u001b[0m\n",
            "            ├── \u001b[01;34mmllib\u001b[0m\n",
            "            │   ├── \u001b[00m__init__.py\u001b[0m\n",
            "            │   ├── \u001b[00mbinary_classification_metrics_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mbisecting_k_means_example.py\u001b[0m\n",
            "            │   ├── \u001b[01;32mcorrelations.py\u001b[0m\n",
            "            │   ├── \u001b[00mcorrelations_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mdecision_tree_classification_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mdecision_tree_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00melementwise_product_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mfpgrowth_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgaussian_mixture_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgaussian_mixture_model.py\u001b[0m\n",
            "            │   ├── \u001b[00mgradient_boosting_classification_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mgradient_boosting_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mhypothesis_testing_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mhypothesis_testing_kolmogorov_smirnov_test_example.py\u001b[0m\n",
            "            │   ├── \u001b[00misotonic_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mk_means_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mkernel_density_estimation_example.py\u001b[0m\n",
            "            │   ├── \u001b[01;32mkmeans.py\u001b[0m\n",
            "            │   ├── \u001b[00mlatent_dirichlet_allocation_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mlinear_regression_with_sgd_example.py\u001b[0m\n",
            "            │   ├── \u001b[01;32mlogistic_regression.py\u001b[0m\n",
            "            │   ├── \u001b[00mlogistic_regression_with_lbfgs_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mmulti_class_metrics_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mmulti_label_metrics_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mnaive_bayes_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mnormalizer_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpca_rowmatrix_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mpower_iteration_clustering_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrandom_forest_classification_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrandom_forest_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[01;32mrandom_rdd_generation.py\u001b[0m\n",
            "            │   ├── \u001b[00mranking_metrics_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mrecommendation_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mregression_metrics_example.py\u001b[0m\n",
            "            │   ├── \u001b[01;32msampled_rdds.py\u001b[0m\n",
            "            │   ├── \u001b[00mstandard_scaler_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mstratified_sampling_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mstreaming_k_means_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mstreaming_linear_regression_example.py\u001b[0m\n",
            "            │   ├── \u001b[00msummary_statistics_example.py\u001b[0m\n",
            "            │   ├── \u001b[00msvd_example.py\u001b[0m\n",
            "            │   ├── \u001b[00msvm_with_sgd_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mtf_idf_example.py\u001b[0m\n",
            "            │   ├── \u001b[00mword2vec.py\u001b[0m\n",
            "            │   └── \u001b[00mword2vec_example.py\u001b[0m\n",
            "            ├── \u001b[00mpagerank.py\u001b[0m\n",
            "            ├── \u001b[00mparquet_inputformat.py\u001b[0m\n",
            "            ├── \u001b[00mpi.py\u001b[0m\n",
            "            ├── \u001b[00msort.py\u001b[0m\n",
            "            ├── \u001b[01;34msql\u001b[0m\n",
            "            │   ├── \u001b[00m__init__.py\u001b[0m\n",
            "            │   ├── \u001b[00marrow.py\u001b[0m\n",
            "            │   ├── \u001b[00mbasic.py\u001b[0m\n",
            "            │   ├── \u001b[00mdatasource.py\u001b[0m\n",
            "            │   ├── \u001b[00mhive.py\u001b[0m\n",
            "            │   ├── \u001b[01;34mstreaming\u001b[0m\n",
            "            │   │   ├── \u001b[00mstructured_kafka_wordcount.py\u001b[0m\n",
            "            │   │   ├── \u001b[00mstructured_network_wordcount.py\u001b[0m\n",
            "            │   │   ├── \u001b[00mstructured_network_wordcount_session_window.py\u001b[0m\n",
            "            │   │   ├── \u001b[00mstructured_network_wordcount_windowed.py\u001b[0m\n",
            "            │   │   └── \u001b[00mstructured_sessionization.py\u001b[0m\n",
            "            │   └── \u001b[00mudtf.py\u001b[0m\n",
            "            ├── \u001b[00mstatus_api_demo.py\u001b[0m\n",
            "            ├── \u001b[01;34mstreaming\u001b[0m\n",
            "            │   ├── \u001b[00m__init__.py\u001b[0m\n",
            "            │   ├── \u001b[00mhdfs_wordcount.py\u001b[0m\n",
            "            │   ├── \u001b[00mnetwork_wordcount.py\u001b[0m\n",
            "            │   ├── \u001b[00mnetwork_wordjoinsentiments.py\u001b[0m\n",
            "            │   ├── \u001b[00mqueue_stream.py\u001b[0m\n",
            "            │   ├── \u001b[00mrecoverable_network_wordcount.py\u001b[0m\n",
            "            │   ├── \u001b[00msql_network_wordcount.py\u001b[0m\n",
            "            │   └── \u001b[00mstateful_network_wordcount.py\u001b[0m\n",
            "            ├── \u001b[00mtransitive_closure.py\u001b[0m\n",
            "            └── \u001b[00mwordcount.py\u001b[0m\n",
            "\n",
            "9 directories, 147 files\n"
          ]
        }
      ],
      "source": [
        "# All examples\n",
        "!tree -I \"__pycache__\" $SPARK_HOME/examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBPlCZvmxEns",
        "outputId": "e7064d6a-6b93-4ad1-aa9b-45b0e4646db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/Users/david/.local/share/virtualenvs/spark-grandes-volumenes-sdSE_HtW/lib/python3.11/site-packages/pyspark/data\u001b[0m\n",
            "├── \u001b[01;34martifact-tests\u001b[0m\n",
            "│   └── \u001b[01;34mcrc\u001b[0m\n",
            "│       ├── \u001b[00mjunitLargeJar.txt\u001b[0m\n",
            "│       └── \u001b[00msmallJar.txt\u001b[0m\n",
            "├── \u001b[01;34mgraphx\u001b[0m\n",
            "│   ├── \u001b[00mfollowers.txt\u001b[0m\n",
            "│   └── \u001b[00musers.txt\u001b[0m\n",
            "├── \u001b[01;34mmllib\u001b[0m\n",
            "│   ├── \u001b[01;34mals\u001b[0m\n",
            "│   │   ├── \u001b[00msample_movielens_ratings.txt\u001b[0m\n",
            "│   │   └── \u001b[00mtest.data\u001b[0m\n",
            "│   ├── \u001b[00mgmm_data.txt\u001b[0m\n",
            "│   ├── \u001b[01;34mimages\u001b[0m\n",
            "│   │   ├── \u001b[00mlicense.txt\u001b[0m\n",
            "│   │   └── \u001b[01;34morigin\u001b[0m\n",
            "│   │       ├── \u001b[01;34mkittens\u001b[0m\n",
            "│   │       │   └── \u001b[00mnot-image.txt\u001b[0m\n",
            "│   │       └── \u001b[00mlicense.txt\u001b[0m\n",
            "│   ├── \u001b[00mkmeans_data.txt\u001b[0m\n",
            "│   ├── \u001b[00mpagerank_data.txt\u001b[0m\n",
            "│   ├── \u001b[00mpic_data.txt\u001b[0m\n",
            "│   ├── \u001b[01;34mridge-data\u001b[0m\n",
            "│   │   └── \u001b[00mlpsa.data\u001b[0m\n",
            "│   ├── \u001b[00msample_binary_classification_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_fpgrowth.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_isotonic_regression_libsvm_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_kmeans_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_lda_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_lda_libsvm_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_libsvm_data.txt\u001b[0m\n",
            "│   ├── \u001b[01;32msample_linear_regression_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_movielens_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_multiclass_classification_data.txt\u001b[0m\n",
            "│   ├── \u001b[00msample_svm_data.txt\u001b[0m\n",
            "│   └── \u001b[00mstreaming_kmeans_data_test.txt\u001b[0m\n",
            "└── \u001b[01;34mstreaming\u001b[0m\n",
            "    └── \u001b[00mAFINN-111.txt\u001b[0m\n",
            "\n",
            "11 directories, 27 files\n"
          ]
        }
      ],
      "source": [
        "# All example datasets\n",
        "!tree $SPARK_HOME/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5HNL29Ju3K6"
      },
      "source": [
        "## Ejemplo2: Contar palabras\n",
        "\n",
        "Ya vimos que PySpark trae ejemplos incluyendo wordcount.py, pero no tenemos un dataset decente para texto. Descarguemos Don Quijote para analizarlo y hagamos nuestro propio contador de palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm33nRhoyKaY",
        "outputId": "d14d3878-72f4-415a-f4e4-4a61c44755af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-21 18:48:52--  https://www.gutenberg.org/cache/epub/996/pg996.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2391728 (2.3M) [text/plain]\n",
            "Saving to: ‘don_quixote.txt’\n",
            "\n",
            "don_quixote.txt     100%[===================>]   2.28M  2.43MB/s    in 0.9s    \n",
            "\n",
            "2024-02-21 18:48:53 (2.43 MB/s) - ‘don_quixote.txt’ saved [2391728/2391728]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/996/pg996.txt -O don_quixote.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiVbhPHTyRoG",
        "outputId": "f65d3a18-ce2f-4424-9e54-00c9bcd5f571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of Don Quixote\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!head -10 don_quixote.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AsNLZqrjy6PD"
      },
      "outputs": [],
      "source": [
        "# Copy into current folder\n",
        "!cp $SPARK_HOME/examples/src/main/python/wordcount.py ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXxqVBbl0Cd_",
        "outputId": "b4a36be8-9946-4d95-9429-1804b319b53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import sys\n",
            "from operator import add\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonWordCount\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "                  .map(lambda x: (x, 1)) \\\n",
            "                  .reduceByKey(add)\n",
            "    output = counts.collect()\n",
            "    for (word, count) in output:\n",
            "        print(\"%s: %i\" % (word, count))\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# wordcount.py but without comments\n",
        "!sed -n 18,42p wordcount.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "R-3tqmIuzFCJ"
      },
      "outputs": [],
      "source": [
        "# Run wordcount.py with output (1: standard output) and error (2: error output) files\n",
        "!spark-submit wordcount.py don_quixote.txt 1>out.txt 2>err.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LdlbW0szbh7",
        "outputId": "241839ee-1ee5-4fbd-a6e0-781f0c7adbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The: 846\n",
            "Project: 80\n",
            "Gutenberg: 23\n",
            "eBook: 4\n",
            "of: 12866\n",
            "Don: 2541\n",
            "Quixote: 1012\n",
            ": 8413\n",
            "This: 97\n",
            "ebook: 2\n"
          ]
        }
      ],
      "source": [
        "!head out.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO33qLRj3rGx"
      },
      "source": [
        "Para trabajar de manera interactiva puedes usar Python directamente en el cuaderno o ejecutar los scripts con el comando `python`, pero toda la configuración de logs y demás variables de ambiente en Spark serán ignoradas (`spark-submit` se encarga de configurar las variables de ambiente de Spark).\n",
        "\n",
        "De todas maneras hagamos una prueba:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5PuA7MEH4LZu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/02/21 18:49:00 WARN Utils: Your hostname, Davids-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.161.51.44 instead (on interface en0)\n",
            "24/02/21 18:49:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/02/21 18:49:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PythonWordCount\").getOrCreate()\n",
        "lines = spark.read.text(\"don_quixote.txt\").rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(add)\n",
        "output = counts.collect()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MMZ6j096X6y",
        "outputId": "372e5217-1a9e-480a-e7b5-54b5fed0143a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 846),\n",
              " ('Project', 80),\n",
              " ('Gutenberg', 23),\n",
              " ('eBook', 4),\n",
              " ('of', 12866),\n",
              " ('Don', 2541),\n",
              " ('Quixote', 1012),\n",
              " ('', 8413),\n",
              " ('This', 97),\n",
              " ('ebook', 2),\n",
              " ('is', 3504),\n",
              " ('for', 4535),\n",
              " ('the', 20933),\n",
              " ('use', 64),\n",
              " ('anyone', 82),\n",
              " ('anywhere', 10),\n",
              " ('in', 6864),\n",
              " ('United', 15),\n",
              " ('States', 8),\n",
              " ('and', 16604)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "## MISIÓN: Contar palabras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "Tu misión si decides aceptarla es cambiar el contador de palabras para que te muestre cuántas palabras comienzan por cada letra, ignorando mayúsculas y minúsculas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PttUSz3B7MFa"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
